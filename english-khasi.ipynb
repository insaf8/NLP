{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8892755,"sourceType":"datasetVersion","datasetId":5347998}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch\n!pip install sacremoses","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-08T10:06:00.067665Z","iopub.execute_input":"2024-07-08T10:06:00.068380Z","iopub.status.idle":"2024-07-08T10:06:26.770081Z","shell.execute_reply.started":"2024-07-08T10:06:00.068346Z","shell.execute_reply":"2024-07-08T10:06:26.769071Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacremoses) (2023.12.25)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses) (1.4.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sacremoses) (4.66.4)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Load your data\nwith open('/kaggle/input/khasi-english-dataset/eng1.txt', 'r', encoding='utf-8') as f:\n    english_sentences = f.readlines()\n\nwith open('/kaggle/input/khasi-english-dataset/khasi1.txt', 'r', encoding='utf-8') as f:\n    khasi_sentences = f.readlines()\n\n# Ensure both files have the same number of lines\nassert len(english_sentences) == len(khasi_sentences)\n\n# Create a DataFrame\ndata = {'en': [en.strip() for en in english_sentences],\n        'kha': [kha.strip() for kha in khasi_sentences]}\n\ndf = pd.DataFrame(data)\n\n# Remove rows with None values\ndf = df.dropna()\n\n# Save to a CSV file\ndf.to_csv('translation_dataset.csv', index=False)\n\nprint(f\"Dataset saved with {len(df)} valid rows.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:06:26.772089Z","iopub.execute_input":"2024-07-08T10:06:26.772431Z","iopub.status.idle":"2024-07-08T10:06:27.685477Z","shell.execute_reply.started":"2024-07-08T10:06:26.772398Z","shell.execute_reply":"2024-07-08T10:06:27.684533Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Dataset saved with 26001 valid rows.\n","output_type":"stream"}]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:06:27.686667Z","iopub.execute_input":"2024-07-08T10:06:27.686929Z","iopub.status.idle":"2024-07-08T10:06:27.707426Z","shell.execute_reply.started":"2024-07-08T10:06:27.686907Z","shell.execute_reply":"2024-07-08T10:06:27.706545Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                      en  \\\n0      Behold , therefore I will bring strangers upon...   \n1      Now when Jesus was risen early the first day o...   \n2      If men strive , and hurt a woman with child , ...   \n3      On the eighth day he sent the people away : an...   \n4      And they of Ephraim shall be like a mighty man...   \n...                                                  ...   \n25996  It is sown in dishonour ; it is raised in glor...   \n25997  That I may know him , and the power of his res...   \n25998  For I testify again to every man that is circu...   \n25999  And shall not uncircumcision which is by natur...   \n26000                                                      \n\n                                                     kha  \n0      ngan wanrah ki nongshun kiba sniew ban tur ïal...  \n1      Hadien ba U Jisu u la mihpat na ka jingïap dan...  \n2      Lada ki rangbah kiba ïashoh ki pynmynsaw ïa ka...  \n3      Ha ka sngi kaba phra u Solomon u phah noh sha ...  \n4      Ki paid Israel kin long kiba khlaiñ kum ki shi...  \n...                                                  ...  \n25996  Haba la tep , ka long kaba ijli bad kaba tlot ...  \n25997  Baroh kaba nga kwah ka long ba ngan ithuh ïa U...  \n25998  Sa shisien pat nga maham ba uno uno u briew ub...  \n25999  Bad kumta ma phi ki Jiw phin shah pynrem ha ki...  \n26000                                                     \n\n[26001 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>kha</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Behold , therefore I will bring strangers upon...</td>\n      <td>ngan wanrah ki nongshun kiba sniew ban tur ïal...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Now when Jesus was risen early the first day o...</td>\n      <td>Hadien ba U Jisu u la mihpat na ka jingïap dan...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>If men strive , and hurt a woman with child , ...</td>\n      <td>Lada ki rangbah kiba ïashoh ki pynmynsaw ïa ka...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>On the eighth day he sent the people away : an...</td>\n      <td>Ha ka sngi kaba phra u Solomon u phah noh sha ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>And they of Ephraim shall be like a mighty man...</td>\n      <td>Ki paid Israel kin long kiba khlaiñ kum ki shi...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25996</th>\n      <td>It is sown in dishonour ; it is raised in glor...</td>\n      <td>Haba la tep , ka long kaba ijli bad kaba tlot ...</td>\n    </tr>\n    <tr>\n      <th>25997</th>\n      <td>That I may know him , and the power of his res...</td>\n      <td>Baroh kaba nga kwah ka long ba ngan ithuh ïa U...</td>\n    </tr>\n    <tr>\n      <th>25998</th>\n      <td>For I testify again to every man that is circu...</td>\n      <td>Sa shisien pat nga maham ba uno uno u briew ub...</td>\n    </tr>\n    <tr>\n      <th>25999</th>\n      <td>And shall not uncircumcision which is by natur...</td>\n      <td>Bad kumta ma phi ki Jiw phin shah pynrem ha ki...</td>\n    </tr>\n    <tr>\n      <th>26000</th>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n<p>26001 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import wandb\n\n# Initialize wandb\nwandb.login(key=\"aca9cf829fa45dbb446b7c861f28378794d2fee7\")","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:06:27.709702Z","iopub.execute_input":"2024-07-08T10:06:27.710551Z","iopub.status.idle":"2024-07-08T10:06:30.451653Z","shell.execute_reply.started":"2024-07-08T10:06:27.710518Z","shell.execute_reply":"2024-07-08T10:06:30.450741Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom transformers import MarianTokenizer, MarianMTModel, Trainer, TrainingArguments\n\n# Load the data from the CSV file\ndf = pd.read_csv('/kaggle/working/translation_dataset.csv')\n\n# Split the data into train and test sets\ntrain_df = df.sample(frac=0.8, random_state=42)\ntest_df = df.drop(train_df.index)\n\n# Convert DataFrames to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Create a DatasetDict\ndataset = DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset\n})\n\n# Initialize the tokenizer and model\nmodel_name = 'Helsinki-NLP/opus-mt-en-mul'\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\n\ndef preprocess_function(examples):\n    inputs = examples['en']\n    targets = examples['kha']\n    \n    valid_inputs = []\n    valid_targets = []\n\n    for i, (inp, tgt) in enumerate(zip(inputs, targets)):\n        if isinstance(tgt, str) and isinstance(inp, str):\n            valid_inputs.append(inp)\n            valid_targets.append(tgt)\n        else:\n            print(f\"Skipping invalid input/target pair at index {i}: {inp}, {tgt}\")\n\n    # Tokenize inputs and targets\n    model_inputs = tokenizer(valid_inputs, max_length=512, truncation=True, padding=\"max_length\")\n    labels = tokenizer(valid_targets, max_length=512, truncation=True, padding=\"max_length\")\n\n    # Ensure labels are correctly aligned with model inputs\n    model_inputs['labels'] = labels['input_ids']\n\n    # Ensure all lists in the dictionary have the same length\n    min_length = min(len(v) for v in model_inputs.values())\n    return {k: v[:min_length] for k, v in model_inputs.items()}\n\n# Tokenize the dataset\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)\n\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/results',\n    evaluation_strategy='epoch',\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=50,\n    weight_decay=0.01,\n    save_total_limit=1,\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['test'],\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:22:05.531610Z","iopub.execute_input":"2024-07-08T10:22:05.532418Z","iopub.status.idle":"2024-07-08T10:25:56.025591Z","shell.execute_reply.started":"2024-07-08T10:22:05.532386Z","shell.execute_reply":"2024-07-08T10:25:56.024518Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20801 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c9ef0ea045b46ca825df0cf8412999a"}},"metadata":{}},{"name":"stdout","text":"Skipping invalid input/target pair at index 881: None, None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35ead3df987e43db860b22c298c32697"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2600' max='2600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2600/2600 03:31, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.898100</td>\n      <td>1.571173</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.447400</td>\n      <td>1.429690</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2600, training_loss=1.8351048161433294, metrics={'train_runtime': 211.7162, 'train_samples_per_second': 196.489, 'train_steps_per_second': 12.281, 'total_flos': 176271497625600.0, 'train_loss': 1.8351048161433294, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:26:28.357171Z","iopub.execute_input":"2024-07-08T10:26:28.357861Z","iopub.status.idle":"2024-07-08T10:26:40.999614Z","shell.execute_reply.started":"2024-07-08T10:26:28.357827Z","shell.execute_reply":"2024-07-08T10:26:40.998431Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.10.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.2.2)\nDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.10.0-py3-none-any.whl (18 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.10.0 sacrebleu-2.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.save_model('/kaggle/working/trained_model')","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:26:41.002258Z","iopub.execute_input":"2024-07-08T10:26:41.002752Z","iopub.status.idle":"2024-07-08T10:26:41.637653Z","shell.execute_reply.started":"2024-07-08T10:26:41.002713Z","shell.execute_reply":"2024-07-08T10:26:41.636436Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import MarianTokenizer, MarianMTModel, pipeline\nfrom datasets import load_metric\n\n# Load the trained model\nmodel_name = 'Helsinki-NLP/opus-mt-en-mul'\nmodel_path = '/kaggle/working/trained_model'\nmodel = MarianMTModel.from_pretrained(model_path)\ntokenizer = MarianTokenizer.from_pretrained(model_name)\n\n# Move the model to CUDA\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Preprocess test dataset\ndef preprocess_test_function(examples):\n    inputs = examples['en']\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    return model_inputs\n\ntokenized_test_dataset = test_dataset.map(preprocess_test_function, batched=True, remove_columns=test_dataset.column_names)\n\n# Generate predictions\ndef generate_predictions(batch):\n    inputs = {k: torch.tensor(v).to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model.generate(**inputs)\n    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\npredictions = []\nreferences = []\n\nfor i in range(len(tokenized_test_dataset)):\n    input_text = tokenized_test_dataset[i]['input_ids']\n    prediction = generate_predictions({'input_ids': [input_text]})\n    predictions.append(prediction[0])\n    references.append(test_dataset[i]['kha'])\n\n# Calculate BLEU score\nbleu = load_metric(\"bleu\")\nresults = bleu.compute(predictions=[pred.split() for pred in predictions],\n                       references=[[ref.split()] for ref in references])\n\nprint(f\"BLEU score: {results['bleu']}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:38:52.956432Z","iopub.execute_input":"2024-07-08T11:38:52.957432Z","iopub.status.idle":"2024-07-08T11:50:43.991857Z","shell.execute_reply.started":"2024-07-08T11:38:52.957398Z","shell.execute_reply":"2024-07-08T11:50:43.990733Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a82675e8c2e649acbb9fec241bc6fbb1"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_35/2212203968.py:40: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  bleu = load_metric(\"bleu\")\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for bleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/bleu/bleu.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"423f5e0980914958848bbb4b7dc6a3bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2763170512d44b339fabee8e6d06ca58"}},"metadata":{}},{"name":"stdout","text":"BLEU score: 0.0031127745988714164\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import MarianTokenizer, MarianMTModel\n\n# Load the trained model and tokenizer\nmodel_name = 'Helsinki-NLP/opus-mt-en-mul'\nmodel_path = '/kaggle/working/trained_model'  # Path to the trained model\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_path)\n\n# Function to translate Khasi text to English\ndef translate_khasi_to_english(khasi_text):\n    # Tokenize the input text\n    inputs = tokenizer(khasi_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n    \n    # Generate translation using the model\n    with torch.no_grad():\n        translated_tokens = model.generate(**inputs)\n    \n    # Decode the tokens to get the translated text\n    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n    return translated_text\n\n# Example usage\nenglish_text = \"i have a red apple\"\nkhasi_translation = translate_khasi_to_english(english_text)\nprint(f\"English: {english_text}\")\nprint(f\"Khasi: {khasi_translation}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:57:45.157892Z","iopub.execute_input":"2024-07-08T11:57:45.158508Z","iopub.status.idle":"2024-07-08T11:57:47.969291Z","shell.execute_reply.started":"2024-07-08T11:57:45.158478Z","shell.execute_reply":"2024-07-08T11:57:47.968305Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"English: i have a red apple\nKhasi: Nga don u masi khyndiat,\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}